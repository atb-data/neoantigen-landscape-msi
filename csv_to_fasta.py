from pathlib import Path
import argparse as ap
import subprocess
import csv
import sys
import os

def csv_to_fasta(csv_path, delimiter=","):
  """Convert a csv-file of the format: <sequence> <name> to a FASTA file."""
  result = ""
  with csv_path.open() as csv_file:
    rd = csv.reader(csv_file, delimiter=delimiter)
    for row in rd:
      result += '> {}\n{}\n'.format(row[1], row[0])
  return result

def write_csv_to_fasta(csv_path, output_path, delimiter=","):
  """Convert a csv-file to FASTA, and write its output to file"""
  with output_path.open("w") as output:
    output.write(csv_to_fasta(csv_path, delimiter=delimiter))

def read_netMHCpan_xls(xls_path):
  """Read an xls-file generated by netMHCpan into a dictionary of thresholds."""
  result = {}
  with xls_path.open() as xls:
    rd = csv.reader(xls, delimiter="\t")
    first_row = next(rd)
    second_row = next(rd)
    hlas = [elem for elem in first_row if elem != ""]
    for row in rd:
      pos = row[0]
      peptide = row[1]
      ID = row[2]
      for idx in range(len(hlas)):
        index = 3 + idx * 5
        hla = hlas[idx]
        peptidescore = [pos, peptide, ID]
        if not hla in result.keys():
          result[hla] = []
        for idy in range(5):
          peptidescore.append(row[index + idy])
        result[hla].append(peptidescore)
  return result

def entity_get_affinity(entity):
  affinity_string = entity[6]
  affinity = None
  try:
    affinity = float(affinity_string)
  except:
    affinity = float("+inf")
  return affinity

def threshold_netMHCpan_results(results, low, high, garbo):
  strong = {}
  weak = {}
  garbage = {}
  for key in results.keys():
    for entity in results[key]:
      peptide = entity[1]
      affinity = entity_get_affinity(entity)
      if affinity < low:
        if not peptide in strong.keys():
          strong[peptide] = {}
        if not key in strong[peptide].keys():
          strong[peptide][key] = []
        strong[peptide][key].append(entity)
      elif affinity < high:
        if not peptide in weak.keys():
          weak[peptide] = {}
        if not key in weak[peptide].keys():
          weak[peptide][key] = []
        weak[peptide][key].append(entity)
      elif affinity < garbo:
        if not peptide in garbage.keys():
          garbage[peptide] = {}
        if not key in garbage[peptide].keys():
          garbage[peptide][key] = []
        garbage[peptide][key].append(entity)
  for peptide in strong.keys():
    for hla in strong[peptide].keys():
      strong[peptide][hla].sort(key = entity_get_affinity)
  for peptide in weak.keys():
    for hla in weak[peptide].keys():
      weak[peptide][hla].sort(key = entity_get_affinity)
  for peptide in garbage.keys():
    for hla in garbage[peptide].keys():
      garbage[peptide][hla].sort(key = entity_get_affinity)

  return {"strong": strong, "weak": weak, "garbage": garbage}

def write_threshold_netMHCpan_results(binders, output_strong, output_weak, output_garbage):
  fieldnames = ["HLA", "Pos", "Peptide", "ID", "core", "icore", "1-log50k", "nM", "Rank"]
  with output_strong.open("w") as output:
    wr = csv.DictWriter(output, fieldnames)
    wr.writeheader()
    for peptide in binders["strong"].keys():
      for hla in binders["strong"][peptide].keys():
        for idx in range(len(binders["strong"][peptide][hla])):
          entity = binders["strong"][peptide][hla][idx]
          wr.writerow({"HLA": hla, "Pos": entity[0], "Peptide": entity[1],
                       "ID": entity[2], "core": entity[3],
                       "icore": entity[4], "1-log50k": entity[5],
                       "nM": entity[6], "Rank": entity[7]})
  with output_weak.open("w") as output:
    wr = csv.DictWriter(output, fieldnames)
    wr.writeheader()
    for peptide in binders["weak"].keys():
      for hla in binders["weak"][peptide].keys():
        for idx in range(len(binders["weak"][peptide][hla])):
          entity = binders["weak"][peptide][hla][idx]
          wr.writerow({"HLA": hla, "Pos": entity[0], "Peptide": entity[1],
                       "ID": entity[2], "core": entity[3],
                       "icore": entity[4], "1-log50k": entity[5],
                       "nM": entity[6], "Rank": entity[7]})
  with output_garbage.open("w") as output:
    wr = csv.DictWriter(output, fieldnames)
    wr.writeheader()
    for peptide in binders["garbage"].keys():
      for hla in binders["garbage"][peptide].keys():
        for idx in range(len(binders["garbage"][peptide][hla])):
          entity = binders["garbage"][peptide][hla][idx]
          wr.writerow({"HLA": hla, "Pos": entity[0], "Peptide": entity[1],
                       "ID": entity[2], "core": entity[3],
                       "icore": entity[4], "1-log50k": entity[5],
                       "nM": entity[6], "Rank": entity[7]})

class NetMHCpan(object):
  """Class-based controller for the NetMHCpan4.0 command line interface."""
  def __init__(self, input, alleles):
    super(NetMHCpan, self).__init__()
    self.input = input
    self.alleles = alleles
    self.home = "$NETMHCpan"
    self.syn = "$NetMHCpan/data/synlist.bin"
    self.tmp = "$TMPDIR/netMHCpan"
    self.pseudo = "$NETMHCpan/data/MHC_pseudo.dat"
    self.seq = None
    self.format = "$NETMHCpan/data/threshold/%s.thr.%s"
    self.version = "$NETMHCpan/data/version"
    self.verbose = False
    self.dirty = False
    self.sort = False
    self.peptide = False
    self.input_type = 'f'
    self.exclude_prefix = False
    self.high = 0.5
    self.low = 2.0
    self.output = None
    self.lengths = [8,9,10,11]
    self.threshold = -99.900002
    self.predict = True

  def _make_command(self):
    result = "netMHCpan"
    result += " -rth {} ".format(self.high)
    result += " -rlt {} ".format(self.low)
    lengths = ",".join(map(str, self.lengths))
    result += " -l {} ".format(lengths)
    if self.verbose:
      result += " -v "
    if self.dirty:
      result += " -dirty "
    if self.sort:
      result += " -s "
    if self.peptide:
      result += " -p "
    if self.output != None:
      result += " -xls -xlsfile {} ".format(self.output)
    if self.seq != None:
      result += " -hlaseq {} ".format(self.seq)
    if self.exclude_prefix:
      result += " -expfix "
    if self.input_type.startswith("f"):
      result += " -inptype 0 "
    else:
      result += " -inptype 1 "
    if self.predict:
      result += " -BA "

    allele_string = ""
    with Path(self.alleles).open() as allele_file:
      allele_array = []
      for line in allele_file:
        allele_array.append(line.strip())
      allele_string = ",".join(allele_array)

    result += " -a {} ".format(allele_string)
    result += " -f {} ".format(self.input)

    return result

  def execute(self):
    subprocess.call(self._make_command(), shell=True)

def main():
  parser = ap.ArgumentParser(description="do the magic!")
  parser.add_argument("inputfile", metavar="INPUT", nargs=1,
                      help="Path to csv files.")
  parser.add_argument("hlapath", metavar="HLAS", nargs=1,
                      help="Path to HLA file.")
  parser.add_argument("outputprefix", metavar="OUTPUT", nargs=1,
                      help="Prefix to the output.")
  args = parser.parse_args()
  path = Path(args.inputfile[0])
  output = args.outputprefix[0]
  hlas = args.hlapath[0]
  for csv_path in path.glob("*.csv"):
    fasta_tmp_str = output + csv_path.name + ".tmp.fasta"
    fasta_tmp_path = Path(fasta_tmp_str)
    write_csv_to_fasta(csv_path, fasta_tmp_path)
    nmhc = NetMHCpan(fasta_tmp_str, hlas)
    nmhc.output = output + csv_path.name + ".tmp.xls"
    nmhc.execute()
    results = read_netMHCpan_xls(Path(output + csv_path.name + ".tmp.xls"))
    binders = threshold_netMHCpan_results(results, 50.0, 500.0, 5000.0)
    write_threshold_netMHCpan_results(
      binders,
      Path(output + csv_path.name + ".strong.csv"),
      Path(output + csv_path.name + ".weak.csv"),
      Path(output + csv_path.name + ".garbage.csv")
    )

if __name__ == "__main__":
    main()
